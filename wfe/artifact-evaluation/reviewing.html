<!-- We currently use <a href="http://www.artifact-eval.org/review-process.html">the following guidelines</a> 
for reviewing artifacts. However, we are preparing a revised version that should be available
around mid-October 2015! 
-->

<p>
This document (<b>V20151015</b>) provides guidelines to review artifacts.

It gradually evolves to define common evaluation criteria based 
on <a href="$#ck_root_page_url#$prior_ae$#ck_page_suffix#$">our past Artifact Evaluations</a>
and your feedback (see this <a href="http://www.slideshare.net/GrigoriFursin/presentation-fursin-aecgoppopp2015">presentation</a>
with an outcome of the past PPoPP/CGO'15 AE).

<!-------------------------------------------------------------------------------------------->
<h2>Reviewing process</h2>

After artifact submission deadline specific to a given
event, AE reviewers will bid on artifacts they would
like to review based on artifact abstract and
check-list, their competencies, and access to specific
hardware and software, while trying to avoid
any conflict of interest. 

Within a few days, AE chairs will 
make a final reviewer selection to ensure at least 2 
reviewers per artifact. 

Reviewers will then have
approximately two weeks to evaluate artifacts 
and provide a report using <a href="$#ck_url_template_pull#$templates/review.txt">this template</a>.


<p>
During rebuttal, authors will be able to address raised issues
and respond to the reviewers.

Finally, reviewers will check if raised issues have been fixed
and will provide the final report. 

Based on all reviewers, AE chairs will make the following final 
assessment of the submitted artifact:

<ul>
 <li>
  <b>+2) significantly exceeded expectations</b>
 </li>
 <li>
  <b>+1) exceeded expectations</b>
 </li>
 <li>
  <b>0) met expectations</b>
 </li>
 <li>
  <b>-1) fell below expectations</b>
 </li>
 <li>
  <b>-2) significantly fell below expectations</b>
 </li>
</ul>

where <b>"met expectations" score or above</b> means that reivewer
managed to evaluate a given artifact possibly with minor problems
that reviewer still managed to solve without authors' assistance.
Such artifact passes evaluation and receives a stamp of approval.

<p>
Note that our goal is not to fail problematic artifacts
but to promote reproducible research via artifact validation and sharing.
Therefore, we allow light communication between reviewers and authors 
whenever there are installation/usage problems.
In such cases, AE chairs serve as a proxy to avoid 
revealing reviewers' identity.

<!-------------------------------------------------------------------------------------------->
<h2>Artifact evaluation</h2>

  The reviewers will need to thoroughly go through authors' guide step-by-step
  to evaluate a given artifact and then describe their experience at each stage 
  (success or failure, encountered problems and how they were possibly solved, 
  and questions or suggestions to the authors), and then give a score 
  on scale -2 .. +2. 

  <p>
  <div style="margin-left: 20px;">

   <table border="1" cellpadding="5" cellspacing="0">
    <tr>
     <td><b>Criteria</b></td>
     <td><b>Score</b></td>
    </tr>
    <tr>
     <td>Documentation</td>
     <td>
      Enough to understand and evaluate artifact?
     </td>
    </tr>
    <tr>
     <td>Packaging</td>
     <td>
      Nothing missing?
     </td>
    </tr>
    <tr>
     <td>Installation procedure</td>
     <td>
      Enough to install and use artifact?
     </td>
    </tr>
    <tr>
     <td>Use case</td>
     <td>
      Enough to validate artifact? 
     </td>
    </tr>
    <tr>
     <td>Expected behavior</td>
     <td>
      Any unexpected artifact behavior (depends
      on the type of artifact such as unexpected output, 
      scalability issues, crashes, performance variation, etc)?
     </td>
    </tr>
    <tr>
     <td>Relevance to paper</td>
     <td>
      How well submitted artifact supports work described in a paper? 
     </td>
    </tr>
    <tr>
     <td>Customization and reusability</td>
     <td>
      <i>Optional and should not be used for overall assessment - mainly used to select distinguished artifact.</i>
      We encourage reviewers to check know whether a given
      artifact can be easily reused and customized. For
      example, can it be used in different environment, 
      with different parameters, under different conditions, 
      or when using different and possibly larger data set 
      (particularly useful to validate whether machine learning based
      techniques are meaningful).
     </td>
    </tr>
    <tr>
     <td>Overall score</td>
     <td>
      Provide explanation of your score and what to improve during rebuttal.
     </td>
    </tr>
   </table>

  </div>

<!-------------------------------------------------------------------------------------------->
<h2>Methodology archive</h2>

To help readers understand which submission/reviewing methodology was used in papers 
with evaluated artifacts we keep track of all past versions:
<ul>
 <li><b>V20151015 (PPoPP'16/CGO'16/ADAPT'16)</b>: 
  <a href="$#ck_url_template_pull#$templates/ae-20151015.tex">LaTeX template</a>,
  <a href="$#ck_root_page_url#$submission-20151015$#ck_page_suffix#$">submission guide</a>,
  <a href="$#ck_root_page_url#$reviewing-20151015$#ck_page_suffix#$">reviewing guide</a>
</ul>
