<center><h1>Frequently Asked Questions</h1>

<small><i>This page continuously evolves and can be modified directly at <a href="https://github.com/ctuning/ck-web-artifact-evaluation/blob/master/wfe/artifact-evaluation/faq.html">GitHub</a></i></small>!
</center>

<!----------------------------------------------------------------------------------------------------->
<h3>Should my software artifacts be open-source?</h3>

No, it is not strictly necessary and you can 
provide your software artifact as a binary.
However, in case of problems, reviewers will not be 
able to fix it and will likely give you a negative score.

<!----------------------------------------------------------------------------------------------------->
<h3>Is Artifact evaluation blind or double-blind?</h3>

AE is a single-blind process, i.e. authors' names are known to the evaluators
(there is no need to hide them since papers are accepted),
but names of evaluators are not known to the authors.
AE chair is usually used as a proxy between the authors and the evaluators
in case of questions and problems.

<p>
In the future, we would like to move to a fully open, community-driven evaluation
which was successfully validated at <a href="http://adapt-workshop.org/motivation2016.html">ADAPT'16</a> -
your comments and ideas are welcome!

<!----------------------------------------------------------------------------------------------------->
<h3>How to pack artifacts?</h3>

We do not have strict requirements at this stage. You can pack 
your artifacts simply in a tar ball, zip file, Virtual Machine or Docker image.
You can also share artifacts via public services such as GitHub or BitBucket.
Please see <a href="$#ck_root_page_url#$submission$#ck_page_suffix#$">our submission guide</a> 
for more details.

<p>
However, from our <a href="http://www.slideshare.net/GrigoriFursin/presentation-fursin-aecgoppopp2015">past Artifact Evaluation</a>, 
the most challenging part is to automate and customize
experimental workflows. It is even worse, if you need
to validate experiments using latest software environment
and hardware (rather than quickly outdated VM and Docker
images). Currently, some ad-hoc scripts are used
to implement such workflows. They are difficult to change
and customize, particularly when an evaluator would like 
to try other compilers, libraries and data sets.

<p>
Therefore, we decided to develop <a href="http://github.com/ctuning/ck">Collective Knowledge Framework (CK)</a> - 
a small, portable and customizable framework to help researchers share their artifacts as reusable Python components 
with a unified JSON API. This approach should help researchers quickly prototype experimental workflows 
(such as multi-objective autotuning) from such components while automatically detecting and resolving
all required software or hardware dependencies. CK is also intended to reduce evaluators' burden
by unifying statistical analysis and predictive analytics (via scikit-learn, R, DNN), 
and enabling interactive reports. Please see examples of a <a href="http://cknowledge.org/repo">live repository</a>,
<a href="http://cknowledge.org/interactive-report">interactive article</a> 
and <a href="https://github.com/ctuning/ck/wiki/Getting_started_guide_clsmith">PLDI'15 CLSmith artifact shared in CK format</a>.
Feel free to contact us, if you would like to use it but need some help to convert your artifacts into CK format.

<!----------------------------------------------------------------------------------------------------->
<h3>Is it possible to provide a remote access to a machine with pre-installed artifacts?</h3>

Only in exceptional cases, i.e. when rare hardware or proprietary software is required,
or VM image is too large, or when you are not authorized to move artifacts outside your organization.
In such case, you will need to send an access information 
to the AE chairs via private email or SMS. They will then pass
this information to the evaluators.

<!----------------------------------------------------------------------------------------------------->
<h3>Can I share commercial benchmarks or software with evaluators?</h3>

Please check the license of your benchmarks, data sets and software. 
In case of any doubts, try to find a free alternative. Note, that we have 
a preliminary agreement with the EEMBC consortium to let authors 
share their EEMBC benchmarks with the evaluators for Artifact Evaluation purposes.

<!----------------------------------------------------------------------------------------------------->
<h3>Should I make my artifacts customizable? How can I plug in benchmarks and datasets from others?</h3>

It is encouraged but not strictly necessary. For example, you can check how it's done in 
<a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">this artifact</a> (distinguished award winner)
from CGO'17 using an open-source <a href="http://github.com/ctuning/ck/wiki">Collective Knowledge framework</a> (CK).
This framework allows you to assemble experimental workflows 
from a growing number of artifacts shared in a customizable and reusable CK format
with a simple JSON API and meta information. You can also share your own artifacts 
(benchmarks, data sets, models, tools) in the CK format.

<!----------------------------------------------------------------------------------------------------->
<h3>Do I have to make my artifacts public if they pass evaluation?</h3>

You are not obliged to make your artifacts public (particularly in case of commercial artifacts).
Nevertheless, we encourage you to make your artifacts publicly available upon publication 
of the proceedings (for example, by including them as "source materials"
in the Digital Library) as a part of <a href="http://dl.acm.org/citation.cfm?id=2618142">our vision for collaborative and reproducible
computer engineering</a>. 

<p>
Furthermore, if you have your artifacts already publicly available at the time
of submission, you may profit from the "public review" option, where you are engaged
directly with the community to discuss, evaluate and use your software. See such
examples <a href="http://cTuning.org/ae/artifacts.html">here</a> (search for "example of public evaluation).

<!----------------------------------------------------------------------------------------------------->
<h3>How to report and compare empirical results?</h3>

You should undoubtedly run empirical experiments more than once! 
There is no universal recipe how many times you should repeat your empirical experiment 
since it heavily depends on the type of your experiments, machine and environment. 

<p>
From our practical experience on collaborative and empirical autotuning
(<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=IwcnpkwAAAAJ&citation_for_view=IwcnpkwAAAAJ:maZDTaKrznsC">example</a>), 
we usually perform
as many repetitions as needed to "stabilize" expected value 
(by analyzing a histogram of the results). But even reporting
variation of the results (for example, standard deviation) is already a good start.

<p>
Furthermore, we strongly suggest you to pre-record results from your platform
and provide a script to automatically compare new results with the pre-recorded ones
preferably using expected values. This will help evaluators avoid wasting time 
when trying to dig out and validate results in stdout.
For example, see how new results are visualized and compared against the pre-recorded ones
using <a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper/files/618737/ck-aarch64-dashboard.pdf">Collective Knowledge dashboard</a> in this 
<a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">CGO'17 distinguished artifact</a>.

<!----------------------------------------------------------------------------------------------------->
<h3>How to deal with numerical accuracy and instability?</h3>

If the accuracy of your results depends on a given machine, environment and optimizations (for example,
when optimizing BLAS, DNN, etc), you should provide a script/plugin to automatically report unexpected 
loss in accuracy (above provided threshold) as well as any numerical instability.

<!----------------------------------------------------------------------------------------------------->
<h3>How to validate models or algorithm scalability?</h3>

If you present a novel parallel algorithm or some predictive model which should scale 
across a number of cores/processors/nodes, we suggest you 
to provide such an experimental workflow which could automatically detect the underlying topology 
of a user machine (or it can at least be configurable), validate your models or algorithm scalability, 
and report any unexpected behavior. In the future, we expect to use public repositories
of knowledge where results will be automatically validated against the ones continuously shared
by the community (<a href="http://cKnowledge.org/repo">1</a>, <a href="https://arxiv.org/abs/1506.06256">2</a>).

<!----------------------------------------------------------------------------------------------------->
<h3>Is there any page limit for my Artifact Evaluation Appendix?</h3>

There is no limit for the AE Appendix at the time of the submission for Artifact Evaluation,
but there is a 2 page limit for the final AE Appendix in the camera-ready conference paper.
We expect to have a less strict limit in the journals willing to participate in our AE initiative.
