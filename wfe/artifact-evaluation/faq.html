<center><h1>Frequently Asked Questions</h1>

<small><i>This page continuously evolves and can be modified directly at <a href="https://github.com/ctuning/ck-artifact-evaluation/blob/master/wfe/artifact-evaluation/faq.html">GitHub</a></i></small>!
</center>

<p>
<b>If you have questions or suggestions which are not addressed here,
post them to the dedicated <a href="https://groups.google.com/forum/#!forum/artifact-evaluation">AE google group</a>.</b>

<!----------------------------------------------------------------------------------------------------->
<h3>Do I have to open source my software artifacts?</h3>

No, it is not strictly necessary and you can 
provide your software artifact as a binary.
However, in case of problems, reviewers will not be 
able to fix it and will likely give you a negative score.

<!----------------------------------------------------------------------------------------------------->
<h3>Is Artifact evaluation blind or double-blind?</h3>

AE is a single-blind process, i.e. authors' names are known to the evaluators
(there is no need to hide them since papers are accepted),
but names of evaluators are not known to authors.
AE chair is usually used as a proxy between authors and evaluators
in case of questions and problems.

<!----------------------------------------------------------------------------------------------------->
<h3>How to pack artifacts?</h3>

We do not have strict requirements at this stage. You can pack 
your artifacts simply in a tar ball, zip file, Virtual Machine or Docker image.
You can also share artifacts via public services including GitHub, GitLab and BitBucket.

Please see <a href="$#ck_root_page_url#$submission$#ck_page_suffix#$">our submission guide</a> 
for more details.

<!----------------------------------------------------------------------------------------------------->
<h3>Is it possible to provide a remote access to a machine with pre-installed artifacts?</h3>

Only in exceptional cases, i.e. when rare hardware or proprietary software is required,
or VM image is too large, or when you are not authorized to move artifacts outside your organization.
In such case, you will need to send access information 
to the AE chairs via private email or SMS. 
They will then pass this information to the evaluators.

<!----------------------------------------------------------------------------------------------------->
<h3>Can I share commercial benchmarks or software with evaluators?</h3>

Please check the license of your benchmarks, data sets and software. 
In case of any doubts, try to find a free alternative. In fact, 
we strongly suggest you provide a small subset of free benchmarks 
and data sets to simplify evaluation.

Note, that we have a preliminary agreement with the <a href="https://www.eembc.org">EEMBC consortium</a>
to let authors share their EEMBC benchmarks with the evaluators for Artifact Evaluation purposes.

<!----------------------------------------------------------------------------------------------------->
<h3>Can I engage with the community to evaluate my artifacts?</h3>

<p>
Based on the community feedback, we provided an extra option of open evaluations
to let the community validate artifacts which are publicly available 
at GitHub, GitLab, BitBuckets, etc, report issues and help authors 
fix them. 

Note, that in the end, these artifacts still go through traditional
evaluation process via the AE committee. We successfully validated 
at <a href="http://adapt-workshop.org/motivation2016.html">ADAPT'16</a> 
and CGO/PPoPP'17 AE!

<!----------------------------------------------------------------------------------------------------->
<h3>How to automate and customize experiments?</h3>

For our past AE experience, the major difficulty for evaluators is
that nearly every artifact pack has its own ad-hoc scripts and formats
(see our <a href="https://fr.slideshare.net/GrigoriFursin/cgoppopp17-artifact-evaluation-discussion-enabling-open-and-reproducible-research">last AE CGO-PPoPP'17 presentation</a>).

Things get even worse, if someone would like to validate experiments 
using latest software environment and hardware (rather than quickly 
outdated VM and Docker images). Most of the submitted scripts are 
not easy to change, customize or port, particularly when an evaluator 
would like to try other compilers, libraries and data sets.

<p>
That's why we collaborate with <a href="http://acm.org">ACM</a> to unify packing and sharing of artifacts as reusable and customizable components
using <a href="http://cKnowledge.org">Collective Knowledge framework</a> 
(see <a href="https://dl.acm.org/docs/reproducibility.cfm">ACM announcement</a>).

CK helps automate and unify your experiments, 
plug in different compilers, benchmarks, data sets, tools, predictive models to your workflows, 
and unify aggregation and visualization of results. 
Please, check out this CGO'17 article from the University of Cambridge ("Software Prefetching for Indirect Memory Accesses") 
with CK-based experimental workflow which won a distinguished artifact award:
<ul>
  <li><a href="$#ck_root_page_url#$resources/paper-with-distinguished-ck-artifact-and-ae-appendix-cgo2017.pdf">Paper&nbsp;with&nbsp;AE&nbsp;appendix&nbsp;and&nbsp;CK&nbsp;workflow</a>
  <li><a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">Artifacts at GitHub</a>
  <li><a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper/files/618737/ck-aarch64-dashboard.pdf">PDF snapshot of the interactive CK dashboard</a>
  <li><a href="https://michel-steuwer.github.io/About-CK">CK&nbsp;concepts</a>
  <li><a href="https://github.com/ctuning/ck/wiki/Portable-workflows">CK cross-platform package manager</a>
  <li><a href="https://github.com/ctuning/ck/wiki/Artifact-sharing">CK artifact sharing</a>
  <li><a href="https://github.com/dividiti/ck-caffe">CK workflow for collaborative Caffe DNN optimization</a>
</ul>

Note that you are not obliged to use CK since it may only influence "Artifacts Evaluated - Reusable" badge 
but not the overall evaluation. However if you are interested to try CK, 
<a href="http://cTuning.org">cTuning foundation (the AE Steering Committee)</a> 
offers free help to convert your workflows
to the CK format while reusing already existing artifacts 
(for example, see <a href="http://cKnowledge.org/ai-artifacts">reusable AI artifacts</a>)
either before or soon after your submission. 

<!----------------------------------------------------------------------------------------------------->
<h3>Do I have to make my artifacts public if they pass evaluation?</h3>

No, you don't have to (it may be impossible in some cases of commercial artifacts).
Nevertheless, we encourage you to make your artifacts publicly available upon publication 
(for example, by including them as "source materials" in the Digital Library) 
as a part of <a href="http://dl.acm.org/citation.cfm?id=2618142">our vision for collaborative and reproducible
computer engineering</a>. 

<p>
Furthermore, if you have your artifacts already publicly available at the time
of submission, you may profit from the "public review" option, where you are engaged
directly with the community to discuss, evaluate and use your software. See such
examples <a href="http://cTuning.org/ae/artifacts.html">here</a> 
(search for "example of public evaluation").

<!----------------------------------------------------------------------------------------------------->
<h3>How to report and compare empirical results?</h3>

First of all, you should undoubtedly run empirical experiments more than once 
(we still encounter many cases where researchers measure execution time only once)! 
There is no universal recipe how many times you should repeat your empirical experiment 
since it heavily depends on the type of your experiments, machine and environment. 
You should then analyze distribution of execution time as shown in the figure below:

<center><img src="https://raw.githubusercontent.com/ctuning/ck-assets/master/slide/reproducibility/994e7359d7760ab1-cropped.png"></center>

<p>If you have more than one expected value (b), it means that you have several
run-time states on your machine which may be switching during your experiments
(such as adaptive frequency scaling) and you can not reliably compare empirical results.

However, if there is only one expected value for a given experiment (a), 
then you can use it to compare multiple experiments (for example during
autotuning as described 
<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=IwcnpkwAAAAJ&citation_for_view=IwcnpkwAAAAJ:maZDTaKrznsC">here</a>).
 
<p>
You should also report variation of empirical results together with expected values.
Furthermore, we strongly suggest you to pre-record results from your platform
and provide a script to automatically compare new results with the pre-recorded ones
preferably using expected values. This will help evaluators avoid wasting time 
when trying to dig out and validate results from "stdout".
For example, see how new results are visualized and compared against the pre-recorded ones
using <a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper/files/618737/ck-aarch64-dashboard.pdf">CK dashboard</a> 
in the <a href="https://github.com/SamAinsworth/reproduce-cgo2017-paper">CGO'17 distinguished artifact</a>.

<!----------------------------------------------------------------------------------------------------->
<h3>How to deal with numerical accuracy and instability?</h3>

If the accuracy of your results depends on a given machine, environment and optimizations 
(for example, when optimizing BLAS, DNN, etc), you should provide a script/plugin to automatically 
report unexpected loss in accuracy (above provided threshold) as well as any numerical instability.

<!----------------------------------------------------------------------------------------------------->
<h3>How to validate models or algorithm scalability?</h3>

If you present a novel parallel algorithm or some predictive model which should scale 
across a number of cores/processors/nodes, we suggest you 
provide such an experimental workflow which could automatically detect the underlying topology 
of a user machine (or it can at least be configurable), validate your models or algorithm scalability, 
and report any unexpected behavior. In the future, we expect to use public repositories
of knowledge where results will be automatically validated against the ones continuously shared
by the community (<a href="http://cKnowledge.org/repo">1</a>, <a href="https://arxiv.org/abs/1506.06256">2</a>).

<!----------------------------------------------------------------------------------------------------->
<h3>Is there any page limit for my Artifact Evaluation Appendix?</h3>

There is no limit for the AE Appendix at the time of the submission for Artifact Evaluation.

<p>There is a 2 page limit for the AE Appendix in the camera-ready CGO,PPoPP and PACT paper.
There is no page limit for the AE Appendix in the camera-ready SC paper. We also expect 
that there will be no page limits for AE Appendices in the journals willing to participate 
in our AE initiative.
